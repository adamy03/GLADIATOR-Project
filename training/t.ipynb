{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gladiator/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "/opt/anaconda3/envs/gladiator/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "/opt/anaconda3/envs/gladiator/lib/python3.9/runpy.py:127: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/opt/anaconda3/envs/gladiator/lib/python3.9/runpy.py:127: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "MineRL agent is public, connect on port 53166 with Minecraft 1.11\n"
     ]
    }
   ],
   "source": [
    "from minerl.env.malmo import InstanceManager\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from env.pvpbox_specs import PvpBox\n",
    "from env.wrappers import OneVersusOneWrapper\n",
    "from env.wrappers import OpponentStepWrapper\n",
    "\n",
    "from model import Discrete_PPO_net\n",
    "from Agent import Agent\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MultiPvpBox(PvpBox):\n",
    "    # This version of treechop doesn't terminate the episode \n",
    "    # if the other agent quits/dies (or gets the max reward)\n",
    "    # def create_server_quit_producers(self):\n",
    "    #     return []\n",
    "    pass\n",
    "\n",
    "agent_actions = {\"attack\":1, \"left\":1, \"right\":1}\n",
    "num_actions = len(agent_actions)\n",
    "env_spec = MultiPvpBox(agent_count=2)\n",
    "\n",
    "# IF you want to use existing instances use this!\n",
    "# instances = [\n",
    "#     InstanceManager.add_existing_instance(9001),\n",
    "#     InstanceManager.add_existing_instance(9002)]\n",
    "instances = []\n",
    "\n",
    "env = env_spec.make(instances=instances)\n",
    "\n",
    "# hero = Agent(Discrete_PPO_net(num_actions), False)\n",
    "\n",
    "# optimizer = optim.Adam(hero.net.parameters(), lr=1e-4)\n",
    "# loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# value_batch = []\n",
    "# action_batch = []\n",
    "# iterate desired episodes\n",
    "# while True:\n",
    "obs = env.reset()\n",
    "# values = []\n",
    "# actions = []\n",
    "# rewards = []\n",
    "# steps = 0\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     steps += 1\n",
    "#     action, action_distribution, values = hero(obs)\n",
    "\n",
    "#     # actions = env.env.action_space.no_op()\n",
    "#     # for agent in actions:\n",
    "#     #     actions[agent][\"forward\"] = 1\n",
    "#     #     actions[agent][\"attack\"] = 1\n",
    "#     #     actions[agent][\"camera\"] = [0, 0.1]\n",
    "\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent_0': {'pov': array([[[18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           ...,\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16]],\n",
       "   \n",
       "          [[18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           ...,\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16]],\n",
       "   \n",
       "          [[18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           ...,\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16],\n",
       "           [18, 41, 16]],\n",
       "   \n",
       "          ...,\n",
       "   \n",
       "          [[89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           ...,\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45]],\n",
       "   \n",
       "          [[89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           ...,\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45]],\n",
       "   \n",
       "          [[89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           ...,\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45],\n",
       "           [89, 63, 45]]], dtype=uint8),\n",
       "   'life_stats': {'air': array(300),\n",
       "    'food': array(20),\n",
       "    'is_alive': array(True),\n",
       "    'life': array(20.),\n",
       "    'saturation': array(5.),\n",
       "    'score': array(0),\n",
       "    'xp': array(0)}},\n",
       "  'agent_1': {'pov': array([[[21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           ...,\n",
       "           [42, 29, 13],\n",
       "           [42, 29, 13],\n",
       "           [21, 15,  7]],\n",
       "   \n",
       "          [[21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           ...,\n",
       "           [42, 29, 13],\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7]],\n",
       "   \n",
       "          [[21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           ...,\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7],\n",
       "           [21, 15,  7]],\n",
       "   \n",
       "          ...,\n",
       "   \n",
       "          [[64, 41, 27],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           ...,\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19]],\n",
       "   \n",
       "          [[47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           ...,\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19]],\n",
       "   \n",
       "          [[47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           ...,\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19],\n",
       "           [47, 28, 19]]], dtype=uint8),\n",
       "   'life_stats': {'air': array(300),\n",
       "    'food': array(20),\n",
       "    'is_alive': array(True),\n",
       "    'life': array(20.),\n",
       "    'saturation': array(5.),\n",
       "    'score': array(0),\n",
       "    'xp': array(0)}}},\n",
       " {'agent_0': 0.0, 'agent_1': 0.0},\n",
       " False,\n",
       " {'agent_0': {}, 'agent_1': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action, action_distribution, values = hero(obs)\n",
    "# obs[\"agent_0\"][\"life_stats\"]\n",
    "# env.action_space.noop()\n",
    "env.step(env.action_space.noop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yj/w4y5s1bd3z55tr6692mv9g5c0000gn/T/ipykernel_8472/1816565701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Gladiator/GLADIATOR-Project/training/../env/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, hero_action)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mhero_ac_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhero_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdual_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhero_ac_str\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhero_ac_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdual_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Gladiator/GLADIATOR-Project/training/../env/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# convert to pytorch and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pov\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__np_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pov\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__np_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agent_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Gladiator/GLADIATOR-Project/training/../env/wrappers.py\u001b[0m in \u001b[0;36m__np_transform\u001b[0;34m(self, np_array)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__np_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"\"\"convert numpy array to pytorch tensor\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mflip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gladiator/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(m, axis)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "obs, reward, done, info = env.step(action)\n",
    "while True:\n",
    "    value_batch = []\n",
    "    action_batch = []\n",
    "    action_distribution_batch = []\n",
    "    adv_batch = []\n",
    "    # obtain batch\n",
    "    for i in range(BATCH_SIZE):\n",
    "        obs = env.reset()\n",
    "        values = []\n",
    "        actions_taken = []\n",
    "        action_distributions = []\n",
    "        rewards = []\n",
    "        steps = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            # get hero action\n",
    "            action, action_distribution, value = hero(obs)\n",
    "            # append items\n",
    "            actions_taken.append(action)\n",
    "            action_distributions.append(action_distribution)\n",
    "            values.append(value)\n",
    "\n",
    "            # step with action\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            # print(reward)\n",
    "\n",
    "        value_batch.append(values)\n",
    "        action_batch.append(actions_taken)\n",
    "        action_distribution_batch.append(action_distributions)\n",
    "        adv_batch.append(GAE_adv(rewards, values, DISCOUNT, LAMBDA))\n",
    "\n",
    "    # train on batch\n",
    "    for i in range(EPOCHS):\n",
    "        action_batch = torch.FloatTensor(action_batch)\n",
    "        exit()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4273b4bab5e5449faae8d739806195113c0ae50dae388bc0556285a0a4600145"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gladiator': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
